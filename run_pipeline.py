# -*- coding: utf-8 -*-
"""Sight2Sound_CaptioningTool_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a89-S__YedLzX5PlsZqlDFO7PyrMrcUt

**Project Workflow Summary**



**Objective:**


**Encoder**


**Decoder**





Pickle file contains the image id and the text associated with the image.

Eg: '319847657_2c40e14113.jpg#0\tA girl in a purple shirt hold a pillow .
1. 319847657_2c40e14113.jpg -> image name
2. #0 -> Caption ID
3. \t  -> separator between Image name and Image Caption
A girl in a purple shirt hold a pillow . -> Image Caption

Corresponding image wrt image name can be found in the image dataset folder.
Image dataset Folder : https://www.kaggle.com/waelboussbat/flickr8ksau/
Images folder can be extracted from above location





🔁 Example Process
Image of cat on couch →
Then: “A cat” → model predicts: “sitting”
Then: “A cat sitting” → model predicts: “on”
"""

#!pip install cv2
!pip install opencv-python

import numpy as np
import pandas as pd
import cv2
import os
from glob import glob


# Download latest version

print("Path to dataset files:", path)
#from google.colab import drive
#drive.mount('/content/drive')

"""**Download MSCOCO Dataset**"""

# Download COCO 2017 subset dataset

print("MSCOCO subset downloaded at:", coco_path)

import json
import os


for item in data['annotations']:
    image_id = f"{item['image_id']:012d}.jpg"

# Sample output


"""# **Image Preprocessing**
1. "Flicker8k_Dataset" must contain all the image files without subfoldering
2. Load Images
3. Plot sample images

#### Reading images
"""

import glob
images_path = '/kaggle/input/flickr8k/Images/'
images = glob.glob(images_path+'*.jpg')
len(images)

"""#### Showing 5 images name"""

images[:5]

"""#### Plotting 4 images from the image dataset"""

import matplotlib.pyplot as plt

for i in range(4):
    plt.figure()
    img = cv2.imread(images[i])
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.imshow(img)


##### Create resnet50 instance
"""

!pip install tensorflow

img_width, img_height = 224, 224
                      include_top=False,
                      pooling='avg',
                      input_shape=(img_height, img_width, 3))

"""##### Create model using keras"""

last = incept_model.layers[-2].output #second last layer of the model
modele.summary()


from keras.preprocessing import image

count = 0
for i in images:
    img = image.load_img(i, target_size=(img_width, img_height)) #size= 224*224 expected by RestNet50
    img_data = image.img_to_array(img) #Converts the image into a NumPy array (i.e., raw pixel values).
    img_data = np.expand_dims(img_data, axis=0) #Adds an extra dimension to make it a batch of 1 image
    #predict the result

    #count += 1
    #if count > 2500:
    #    break


"""**Load Features for MSCOCCO images**"""

mscoco_images_dir = os.path.join(mscoco_dir, "train2017")

from keras.preprocessing import image

    count = 0

    for img_name in image_keys:
        img_path = os.path.join(image_dir, img_name)
        if not os.path.exists(img_path): continue

        img = image.load_img(img_path, target_size=(224, 224))
        img_data = image.img_to_array(img)
        img_data = np.expand_dims(img_data, axis=0)


        count += 1
        if count >= limit:
            break



"""

import pickle

# Path to your downloaded file

# Parse and convert to dictionary
with open(input_txt_path, "r", encoding="utf-8") as f:
    for line in f:
        if '\t' in line:
            img_id = img_id_full.split('#')[0]

# Save to pickle
with open(output_pkl_path, "wb") as f:

print(f" Saved as: {output_pkl_path}")

# Load the pickle file

# Print first 3 items
    print(f"{i+1}. Image: {img_name}")
        print(f"   Caption {j+1}: {cap}")
    print()  # Line break between items
    if i == 2:  # Only first 3 items
        break




        ]


"""# **Visualization**
"""

import matplotlib.pyplot as plt

for i in range(2):
    plt.figure()
    img_name = images[i]
    print(img_name)
    img = cv2.imread(img_name)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.imshow(img)

"""##### Method to add start & end tag
"""

def preprocessed(txt):
    modified = txt.lower()
    print(modified)
    return modified

    for vv in v:

"""# **Prepare Vocabulary**

1. Prepare count_words dictionary


"""

count_words = {}
    for v in vv:
        for word in v.split():
            if word not in count_words:
                count_words[word] = 0
            else:
                count_words[word] += 1


THRESH = -1
count = 1
new_dict = {}
for k,v in count_words.items():
    if count_words[k] > THRESH:
        new_dict[k] = count
        count += 1
len(new_dict)

# Handle unknown words predectibely
new_dict['<OUT>'] = len(new_dict)

#Backup

print(len(count_words))
print(len(new_dict))

"""**##### Encoding Captions for model to learn patterns from numerical data.**"""

    for v in vv:
        encoded = []
        for word in v.split():
            if word not in new_dict:
                encoded.append(new_dict['<OUT>'])
            else:
                encoded.append(new_dict[word])



"""# **Build Generator Function**
3. Data from this section to be used in modelling
"""

from tensorflow.keras.utils import to_categorical #one hot conversion

MAX_LEN = 0
    for v in vv:
        print(v)
        if len(v) > MAX_LEN:
            MAX_LEN = len(v) #potential bug- sushil also last line
            MAX_LEN = len(v)
            print(v)
print(MAX_LEN)

Batch_size = 5000
VOCAB_SIZE = len(new_dict)

    n_samples = 0

    X = []
    y_in = []
    y_out = []

        for v in vv:
            for i in range(1, len(v)):
                X.append(photo[k][0])

                in_seq= [v[:i]]
                out_seq = v[i]

                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]
                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]

                y_in.append(in_seq)
                y_out.append(out_seq)

    return X, y_in, y_out



print(len(X), len(y_in), len(y_out))

X = np.array(X)
y_in = np.array(y_in, dtype='float64')
y_out = np.array(y_out, dtype='float64')

print(X.shape, y_in.shape, y_out.shape)
print(len(X[5]))

1. Hyper parameter tuning
2. Saving the model
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Dropout
#from tensorflow.keras.layers.merge import add
from tensorflow.keras.layers import concatenate
from tensorflow.keras import layers
from tensorflow.keras import regularizers

import tensorflow as tf
# def hypertuning(params):
#     print("===>", params)
#     embedding_size = params[0]
#     max_len = MAX_LEN
#     vocab_size = len(new_dict)

#     image_model = Sequential()

#     image_model.add(Dense(embedding_size, input_shape=(1000,), activation='relu',kernel_regularizer=regularizers.l1(0.01)))
# #     image_model.add(Dropout(0.25))
#     image_model.add(RepeatVector(max_len))

# #     image_model.summary()

#     language_model = Sequential()

#     language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))
# #     language_model.add(Dropout(0.10))
#     language_model.add(TimeDistributed(Dense(embedding_size)))

# #     language_model.summary()

#     #Concat image and language model
#     conca = Concatenate()([image_model.output, language_model.output])
#     x = Dense(vocab_size)(x)
#     out = Activation('softmax')(x)

#     #Set Learning rate
#     opt = ""
#     if (params[7] == "Adam"):
#         opt = tf.keras.optimizers.Adam(learning_rate=0.01)
#     elif (params[7] == "SGD"):
#         opt = tf.keras.optimizers.SGD(learning_rate=0.01)
#     elif (params[7] == "RMSprop"):
#         opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)


#     #Compiling the model
#     model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
# #     model.summary()

#     #Fitting the model

#     return history, model

# hparams1 = [64, 128, 64, 128, 256, 512, 1024, 'RMSprop', 256, 10]
# hparams2 = [128, 256, 128, 256, 512, 256, 512, 'Adam', 256, 10]
# hparams3 = [128, 256, 128, 256, 512, 512, 512, 'SGD', 128, 10]
# hparams4 = [128, 256, 128, 256, 512, 512, 512, 'RMSprop', 128, 10]
# hparams5 = [128, 256, 128, 256, 512, 512, 512, 'RMSprop', 512, 10]

# h1, model1 = hypertuning(hparams1)
# h2, model2 = hypertuning(hparams2)
# h3, model3 = hypertuning(hparams3)
# h4, model4 = hypertuning(hparams4)
# h5, model5 = hypertuning(hparams5)

inv_dict = {v:k for k, v in new_dict.items()}


#sushil


# Parameters
embedding_size = 128
max_len = MAX_LEN
vocab_size = len(new_dict)

# Step 1: Define image input model
image_dense = Dense(embedding_size, activation='relu')(image_input)
image_repeated = RepeatVector(max_len)(image_dense)  # Repeat to match sequence length

#  Step 2: Define language input model
text_input = Input(shape=(max_len,), name="text_input")  # Input sequence
text_embedded = Embedding(input_dim=vocab_size, output_dim=embedding_size)(text_input)
text_time_distributed = TimeDistributed(Dense(embedding_size))(text_lstm)

#  Step 3: Merge the image and language streams
merged = Concatenate()([image_repeated, text_time_distributed])
x = Dense(vocab_size)(x)
output = Activation('softmax')(x)

#  Step 4: Define and compile the final model
model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])

model.summary()


#  Step 5: Train the model

def getImage(x):

    # test_img_path = images[x]

    test_img = cv2.imread(x)#test_img_path)
    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)

    test_img = cv2.resize(test_img, (224,224))

    test_img = np.reshape(test_img, (1,224,224,3))

    return test_img

"""# **Predictions**

**Download any image from google images and name that images "test1.jpg"
Keep the file in location './testimages/test1.jpg' from where code is running**
"""

test_img_path = '/content/img-6.jpeg'


test_img = cv2.imread(test_img_path)
test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)



count = 0
while count < 25:
    count += 1

    encoded = []
    for i in text_inp:
        encoded.append(new_dict[i])

    encoded = [encoded]

    encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)



    sampled_word = inv_dict[prediction]


        break
    text_inp.append(sampled_word)

plt.figure()
plt.imshow(test_img)


1	Take video as input

2	Extract key frames where content changes



"""

#import cv2
#import numpy as np
#from tensorflow.keras.preprocessing import image
#from tensorflow.keras.preprocessing.sequence import pad_sequences
#import matplotlib.pyplot as plt

# --- Helper: Convert BGR to resized normalized tensor ---
def getImage_from_frame(frame):
    img = cv2.resize(frame, (img_width, img_height))
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    return img

# --- Helper: Extract key frames from a video based on frame difference ---
def extract_key_frames(video_path, diff_threshold=200, max_frames=10):
    cap = cv2.VideoCapture(video_path)
    key_frames = []
    prev_frame = None
    count = 0

    while True:
        if len(key_frames) >= max_frames:
          break  # Stop if you've collected enough key frames

        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if prev_frame is None:
            prev_frame = gray
            key_frames.append(frame)
            continue

        diff = cv2.absdiff(prev_frame, gray)
        non_zero_count = np.count_nonzero(diff)

        if non_zero_count > diff_threshold * 1000:  # adjust threshold as needed
            key_frames.append(frame)
            prev_frame = gray

        count += 1
    cap.release()
    return key_frames


    count = 0

    while count < 25:
        count += 1
        encoded = [new_dict.get(w, new_dict['<OUT>']) for w in text_inp]
        encoded = pad_sequences([encoded], padding='post', truncating='post', maxlen=MAX_LEN)

        if int(prediction) not in inv_dict:
            print(f" Skipping unknown prediction: {prediction}")
            sampled_word = '<UNK>'
        else:
            sampled_word = inv_dict[int(prediction)]

            break
        text_inp.append(sampled_word)



# --- Run for each frame ---
video_path = '/content/video-1.mp4'  #  Set the video file path
frames = extract_key_frames(video_path)

for idx, frame in enumerate(frames):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    plt.figure()
    plt.imshow(rgb)
    plt.axis('off')
    plt.show()

"""**gTTS Integration**"""

!pip install gTTS
from gtts import gTTS
audio_output_path = 'final_audio.mp3'

    tts = gTTS(text=narration_text, lang='en')
else: